# ğŸ› ï¸ æŠ€æœ¯æ ˆè¯¦è§£

## ğŸ“‹ ç›®å½•
1. [æ ¸å¿ƒæ¡†æ¶](#æ ¸å¿ƒæ¡†æ¶)
2. [AIå’ŒLLM](#aiå’Œllm)
3. [æ•°æ®åº“å’Œå­˜å‚¨](#æ•°æ®åº“å’Œå­˜å‚¨)
4. [å·¥å…·å’Œåº“](#å·¥å…·å’Œåº“)
5. [éƒ¨ç½²å’Œè¿ç»´](#éƒ¨ç½²å’Œè¿ç»´)

---

## æ ¸å¿ƒæ¡†æ¶

### FastAPI (v0.109.0)
**ç”¨é€”**ï¼šWebæ¡†æ¶å’ŒREST API

**ç‰¹ç‚¹**ï¼š
- å¼‚æ­¥æ”¯æŒï¼ˆasync/awaitï¼‰
- è‡ªåŠ¨APIæ–‡æ¡£ç”Ÿæˆï¼ˆSwagger UIï¼‰
- æ•°æ®éªŒè¯ï¼ˆPydanticï¼‰
- é«˜æ€§èƒ½ï¼ˆåŸºäºStarletteï¼‰

**å…³é”®ç”¨æ³•**ï¼š
```python
from fastapi import FastAPI
from fastapi.middleware.cors import CORSMiddleware

app = FastAPI()

# CORSé…ç½®
app.add_middleware(
    CORSMiddleware,
    allow_origins=["http://localhost:8501"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

@app.post("/api/analyze")
async def analyze_culture(request: AnalyzeRequest):
    return await culture_analyst.analyze(request.village_info)
```

**æ–‡ä»¶ä½ç½®**ï¼š`backend/main.py`, `backend/api/routes.py`

---

### Streamlit (å‰ç«¯)
**ç”¨é€”**ï¼šå¿«é€Ÿæ„å»ºæ•°æ®åº”ç”¨UI

**ç‰¹ç‚¹**ï¼š
- æ— éœ€å‰ç«¯æ¡†æ¶ï¼Œçº¯Python
- çƒ­é‡è½½
- å†…ç½®ç»„ä»¶åº“
- å¿«é€ŸåŸå‹å¼€å‘

**å…³é”®ç”¨æ³•**ï¼š
```python
import streamlit as st

st.title("ä¹¡æ‘å¢™ç»˜AIç”Ÿæˆç³»ç»Ÿ")
village_name = st.text_input("æ‘è½åç§°")
if st.button("åˆ†æ"):
    response = requests.post("http://localhost:8000/api/analyze", ...)
    st.write(response.json())
```

**æ–‡ä»¶ä½ç½®**ï¼š`frontend/app.py`

---

## AIå’ŒLLM

### CrewAI (v0.1.26)
**ç”¨é€”**ï¼šå¤šAgentå·¥ä½œæµç¼–æ’

**ç‰¹ç‚¹**ï¼š
- å®šä¹‰Agentè§’è‰²å’Œèƒ½åŠ›
- ç¼–æ’Agentä¹‹é—´çš„åä½œ
- æ”¯æŒåˆ†æ­¥æ‰§è¡Œå’Œå®Œæ•´å·¥ä½œæµ
- è¿›åº¦å›è°ƒæœºåˆ¶

**å…³é”®æ¦‚å¿µ**ï¼š
```python
from crewai import Agent, Task, Crew

# å®šä¹‰Agent
analyst = Agent(
    role="æ–‡åŒ–ç ”ç©¶ä¸“å®¶",
    goal="åˆ†æä¹¡æ‘æ–‡åŒ–ç‰¹è‰²",
    backstory="èµ„æ·±çš„ä¹¡æ‘æ–‡åŒ–ç ”ç©¶è€…",
    tools=[search_tool, analysis_tool]
)

# å®šä¹‰Task
analyze_task = Task(
    description="åˆ†ææ‘è½çš„æ–‡åŒ–ç‰¹è‰²",
    agent=analyst,
    expected_output="è¯¦ç»†çš„æ–‡åŒ–åˆ†ææŠ¥å‘Š"
)

# ç¼–æ’Crew
crew = Crew(
    agents=[analyst],
    tasks=[analyze_task],
    verbose=True
)

result = crew.kickoff()
```

**æ–‡ä»¶ä½ç½®**ï¼š`backend/agents/crew_manager.py`

---

### LangChain (v0.1.0)
**ç”¨é€”**ï¼šLLMåº”ç”¨æ¡†æ¶

**ç‰¹ç‚¹**ï¼š
- Agentå®ç°å’Œå·¥å…·è°ƒç”¨
- Memoryç®¡ç†
- Chainç¼–æ’
- æç¤ºæ¨¡æ¿

**å…³é”®ç”¨æ³•**ï¼š
```python
from langchain.agents import Tool, AgentExecutor, initialize_agent
from langchain.memory import ConversationBufferMemory
from langchain.llms import OpenAI

# å®šä¹‰å·¥å…·
tools = [
    Tool(
        name="search_knowledge",
        func=search_villages,
        description="æœç´¢æ‘è½çŸ¥è¯†åº“"
    )
]

# åˆ›å»ºAgent
memory = ConversationBufferMemory(memory_key="chat_history")
agent = initialize_agent(
    tools,
    llm,
    agent="zero-shot-react-description",
    memory=memory
)

# æ‰§è¡Œ
result = agent.run("åˆ†æè¥¿é€’æ‘çš„æ–‡åŒ–ç‰¹è‰²")
```

**æ–‡ä»¶ä½ç½®**ï¼š`backend/agents/`

---

### é€šä¹‰åƒé—® (Qwen)
**ç”¨é€”**ï¼šå¤§è¯­è¨€æ¨¡å‹

**æ¨¡å‹**ï¼š`qwen-plus`

**ç‰¹ç‚¹**ï¼š
- ä¸­æ–‡ç†è§£èƒ½åŠ›å¼º
- æ”¯æŒé•¿æ–‡æœ¬
- ä½å»¶è¿Ÿ
- æˆæœ¬ä½

**APIè°ƒç”¨**ï¼š
```python
from dashscope import Generation

response = Generation.call(
    model='qwen-plus',
    messages=[
        {'role': 'system', 'content': 'ä½ æ˜¯ä¸€ä½æ–‡åŒ–ç ”ç©¶ä¸“å®¶'},
        {'role': 'user', 'content': 'åˆ†æè¥¿é€’æ‘çš„æ–‡åŒ–ç‰¹è‰²'}
    ],
    temperature=0.7,
    max_tokens=2000
)

print(response.output.text)
```

**æ–‡ä»¶ä½ç½®**ï¼š`backend/services/llm_service.py`

---

### é€šä¹‰ä¸‡ç›¸ (Wanx)
**ç”¨é€”**ï¼šæ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆ

**æ¨¡å‹**ï¼š`wanx-v1`

**ç‰¹ç‚¹**ï¼š
- æ”¯æŒä¸­æ–‡Prompt
- å¤šç§é£æ ¼
- é«˜è´¨é‡è¾“å‡º
- å¿«é€Ÿç”Ÿæˆ

**APIè°ƒç”¨**ï¼š
```python
from dashscope import ImageSynthesis

response = ImageSynthesis.call(
    model='wanx-v1',
    prompt='A traditional Chinese mural painting...',
    negative_prompt='low quality, blurry',
    style='<chinese-painting>',
    size='1024*1024',
    n=1
)

image_url = response.output.results[0].url
```

**æ–‡ä»¶ä½ç½®**ï¼š`backend/services/image_service.py`

---

## æ•°æ®åº“å’Œå­˜å‚¨

### ChromaDB (v0.4.22)
**ç”¨é€”**ï¼šå‘é‡æ•°æ®åº“

**ç‰¹ç‚¹**ï¼š
- è½»é‡çº§ï¼Œæ— éœ€å•ç‹¬éƒ¨ç½²
- æ”¯æŒå‘é‡ç›¸ä¼¼åº¦æœç´¢
- æ”¯æŒå…ƒæ•°æ®è¿‡æ»¤
- å†…ç½®Embeddingæ¨¡å‹

**å…³é”®ç”¨æ³•**ï¼š
```python
import chromadb

# åˆå§‹åŒ–
client = chromadb.Client()

# åˆ›å»ºé›†åˆ
collection = client.create_collection(
    name="villages_knowledge",
    metadata={"hnsw:space": "cosine"}
)

# æ·»åŠ æ–‡æ¡£
collection.add(
    ids=["village_001"],
    documents=["è¥¿é€’æ‘ä½äºå®‰å¾½çœé»„å±±å¸‚..."],
    metadatas=[{"name": "è¥¿é€’æ‘"}]
)

# æœç´¢
results = collection.query(
    query_texts=["å¾½æ´¾å»ºç­‘ç‰¹è‰²"],
    n_results=3
)
```

**æ–‡ä»¶ä½ç½®**ï¼š`backend/services/chromadb_service.py`

---

### Sentence Transformers
**ç”¨é€”**ï¼šæ–‡æœ¬å‘é‡åŒ–

**æ¨¡å‹**ï¼š`paraphrase-multilingual-MiniLM-L12-v2`

**ç‰¹ç‚¹**ï¼š
- å¤šè¯­è¨€æ”¯æŒ
- è½»é‡çº§
- é«˜æ•ˆç‡
- è¯­ä¹‰ç†è§£

**ç”¨æ³•**ï¼š
```python
from sentence_transformers import SentenceTransformer

model = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')

# å‘é‡åŒ–
embeddings = model.encode([
    "è¥¿é€’æ‘çš„å¾½æ´¾å»ºç­‘",
    "ä¼ ç»Ÿæ–‡åŒ–ç‰¹è‰²"
])

# è®¡ç®—ç›¸ä¼¼åº¦
from sklearn.metrics.pairwise import cosine_similarity
similarity = cosine_similarity([embeddings[0]], [embeddings[1]])
```

**é›†æˆ**ï¼šChromaDBå†…ç½®ä½¿ç”¨

---

## å·¥å…·å’Œåº“

### Pydantic (v2.x)
**ç”¨é€”**ï¼šæ•°æ®éªŒè¯å’Œè®¾ç½®ç®¡ç†

**ç‰¹ç‚¹**ï¼š
- ç±»å‹æ£€æŸ¥
- è‡ªåŠ¨éªŒè¯
- é”™è¯¯æç¤ºæ¸…æ™°
- æ”¯æŒå¤æ‚æ•°æ®ç»“æ„

**å…³é”®ç”¨æ³•**ï¼š
```python
from pydantic import BaseModel, Field, validator

class VillageInfo(BaseModel):
    name: str = Field(..., description="æ‘è½åç§°")
    location: str = Field(..., description="åœ°ç†ä½ç½®")
    industry: str = Field(..., description="ä¸»è¦äº§ä¸š")
    history: str = Field(..., description="å†å²èƒŒæ™¯")
    
    @validator('name')
    def name_not_empty(cls, v):
        if not v.strip():
            raise ValueError('æ‘è½åç§°ä¸èƒ½ä¸ºç©º')
        return v

# ä½¿ç”¨
village = VillageInfo(
    name="è¥¿é€’æ‘",
    location="å®‰å¾½çœé»„å±±å¸‚",
    industry="æ—…æ¸¸",
    history="æ˜æ¸…å¤æ‘è½"
)
```

**æ–‡ä»¶ä½ç½®**ï¼š`backend/api/models.py`, `backend/core/config.py`

---

### Loguru
**ç”¨é€”**ï¼šæ—¥å¿—ç®¡ç†

**ç‰¹ç‚¹**ï¼š
- ç®€æ´çš„API
- è‡ªåŠ¨æ–‡ä»¶è½®è½¬
- å½©è‰²è¾“å‡º
- ç»“æ„åŒ–æ—¥å¿—

**å…³é”®ç”¨æ³•**ï¼š
```python
from loguru import logger

# é…ç½®
logger.add(
    "logs/backend.log",
    rotation="500 MB",
    retention="10 days",
    level="INFO"
)

# ä½¿ç”¨
logger.info("åº”ç”¨å¯åŠ¨")
logger.error("å‘ç”Ÿé”™è¯¯", exc_info=True)
logger.debug("è°ƒè¯•ä¿¡æ¯")
```

**æ–‡ä»¶ä½ç½®**ï¼š`backend/main.py`

---

### Python-dotenv
**ç”¨é€”**ï¼šç¯å¢ƒå˜é‡ç®¡ç†

**ç‰¹ç‚¹**ï¼š
- ä».envæ–‡ä»¶åŠ è½½
- æ”¯æŒæ³¨é‡Š
- ç±»å‹è½¬æ¢

**å…³é”®ç”¨æ³•**ï¼š
```python
from dotenv import load_dotenv
import os

load_dotenv()

api_key = os.getenv("DASHSCOPE_API_KEY")
port = int(os.getenv("BACKEND_PORT", 8000))
```

**æ–‡ä»¶ä½ç½®**ï¼š`backend/core/config.py`

---

### Requests
**ç”¨é€”**ï¼šHTTPå®¢æˆ·ç«¯

**ç‰¹ç‚¹**ï¼š
- ç®€æ´çš„API
- è‡ªåŠ¨å¤„ç†JSON
- è¶…æ—¶æ§åˆ¶
- é‡è¯•æœºåˆ¶

**å…³é”®ç”¨æ³•**ï¼š
```python
import requests

response = requests.post(
    "https://api.example.com/data",
    json={"key": "value"},
    timeout=30,
    headers={"Authorization": "Bearer token"}
)

data = response.json()
```

**æ–‡ä»¶ä½ç½®**ï¼š`backend/services/`

---

## éƒ¨ç½²å’Œè¿ç»´

### Uvicorn
**ç”¨é€”**ï¼šASGIæœåŠ¡å™¨

**ç‰¹ç‚¹**ï¼š
- é«˜æ€§èƒ½
- æ”¯æŒçƒ­é‡è½½
- æ”¯æŒå¤šè¿›ç¨‹
- æ”¯æŒSSL

**å¯åŠ¨å‘½ä»¤**ï¼š
```bash
# å¼€å‘æ¨¡å¼
uvicorn backend.main:app --reload --host 0.0.0.0 --port 8000

# ç”Ÿäº§æ¨¡å¼
uvicorn backend.main:app --workers 4 --host 0.0.0.0 --port 8000
```

---

### Gunicorn
**ç”¨é€”**ï¼šWSGIåº”ç”¨æœåŠ¡å™¨ï¼ˆç”Ÿäº§ç¯å¢ƒï¼‰

**é…ç½®**ï¼š
```bash
gunicorn -w 4 -b 0.0.0.0:8000 backend.main:app
```

---

### Docker
**ç”¨é€”**ï¼šå®¹å™¨åŒ–éƒ¨ç½²

**Dockerfileç¤ºä¾‹**ï¼š
```dockerfile
FROM python:3.10-slim

WORKDIR /app

COPY requirements.txt .
RUN pip install -r requirements.txt

COPY . .

CMD ["uvicorn", "backend.main:app", "--host", "0.0.0.0", "--port", "8000"]
```

---

## ğŸ“Š ä¾èµ–å…³ç³»å›¾

```
FastAPI
â”œâ”€â”€ Starlette (Webæ¡†æ¶)
â”œâ”€â”€ Pydantic (æ•°æ®éªŒè¯)
â””â”€â”€ Uvicorn (ASGIæœåŠ¡å™¨)

CrewAI
â”œâ”€â”€ LangChain (Agentæ¡†æ¶)
â”œâ”€â”€ Pydantic (æ•°æ®æ¨¡å‹)
â””â”€â”€ é€šä¹‰åƒé—® (LLM)

LangChain
â”œâ”€â”€ Pydantic (æ•°æ®æ¨¡å‹)
â”œâ”€â”€ Requests (HTTPå®¢æˆ·ç«¯)
â””â”€â”€ å‘é‡æ•°æ®åº“ (ChromaDB)

ChromaDB
â”œâ”€â”€ Sentence Transformers (Embedding)
â””â”€â”€ SQLite (æœ¬åœ°å­˜å‚¨)

é€šä¹‰åƒé—®/ä¸‡ç›¸
â””â”€â”€ DashScope SDK

Streamlit
â”œâ”€â”€ Requests (HTTPå®¢æˆ·ç«¯)
â””â”€â”€ Pandas (æ•°æ®å¤„ç†)
```

---

## ğŸ”„ ç‰ˆæœ¬å…¼å®¹æ€§

| ç»„ä»¶ | ç‰ˆæœ¬ | Pythonç‰ˆæœ¬ | å¤‡æ³¨ |
|------|------|-----------|------|
| FastAPI | 0.109.0 | 3.8+ | æœ€æ–°ç¨³å®šç‰ˆ |
| CrewAI | 0.1.26 | 3.9+ | å®éªŒæ€§åŠŸèƒ½ |
| LangChain | 0.1.0 | 3.8+ | å¿«é€Ÿè¿­ä»£ |
| ChromaDB | 0.4.22 | 3.8+ | å‘é‡DB |
| Streamlit | 1.28+ | 3.8+ | å‰ç«¯æ¡†æ¶ |
| Pydantic | 2.x | 3.8+ | æ•°æ®éªŒè¯ |
| Python | 3.10+ | - | æ¨èç‰ˆæœ¬ |

---

## ğŸ“¦ å®Œæ•´ä¾èµ–åˆ—è¡¨

```
# æ ¸å¿ƒæ¡†æ¶
fastapi==0.109.0
uvicorn[standard]==0.27.0
starlette==0.35.0

# AIå’ŒLLM
crewai==0.1.26
langchain==0.1.0
dashscope==1.14.0

# æ•°æ®åº“
chromadb==0.4.22
sentence-transformers==2.2.2

# å‰ç«¯
streamlit==1.28.0

# å·¥å…·åº“
pydantic==2.5.0
pydantic-settings==2.1.0
python-dotenv==1.0.0
requests==2.31.0
loguru==0.7.2

# å¼€å‘å·¥å…·
pytest==7.4.3
black==23.12.0
mypy==1.7.1
```

---

## ğŸš€ æ€§èƒ½æŒ‡æ ‡

| ç»„ä»¶ | æ€§èƒ½ | å¤‡æ³¨ |
|------|------|------|
| FastAPI | <100ms | å•ä¸ªè¯·æ±‚ |
| ChromaDBæ£€ç´¢ | <500ms | 1000ä¸ªæ–‡æ¡£ |
| LLMè°ƒç”¨ | 2-5s | é€šä¹‰åƒé—® |
| å›¾åƒç”Ÿæˆ | 10-30s | é€šä¹‰ä¸‡ç›¸ |
| æ€»æµç¨‹ | 15-40s | å®Œæ•´å·¥ä½œæµ |

---

## ğŸ’¾ å­˜å‚¨éœ€æ±‚

| ç»„ä»¶ | å¤§å° | å¤‡æ³¨ |
|------|------|------|
| ChromaDB | ~100MB | 1000ä¸ªæ–‡æ¡£ |
| ç”Ÿæˆå›¾åƒ | ~2MB | å•å¼ 1024x1024 |
| æ—¥å¿—æ–‡ä»¶ | ~50MB | æ¯æœˆ |
| æ¨¡å‹ç¼“å­˜ | ~500MB | Embeddingæ¨¡å‹ |

---

**æœ€åæ›´æ–°**: 2025-10-16  
**ç‰ˆæœ¬**: 1.0.0

